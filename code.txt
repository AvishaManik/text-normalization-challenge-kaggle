from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import gc
import re
from sklearn.model_selection import train_test_split
import keras
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import RMSprop
from matplotlib import pyplot as plt

def make_encoded_space_padded_tokens(data, max_num_features, space_char):
    space_padded_tokens = list()
    for before_value in data:
        space_padded_token = np.ones(max_num_features, dtype=int) * space_char
        before_value_c_str = list(str(before_value))
        for before_value_char, i in zip(before_value_c_str, range(max_num_features)):
            space_padded_token[i] = ord(before_value_char)
        space_padded_tokens.append(space_padded_token)
    return space_padded_tokens

def make_flat_context_windows(data, pad_size, max_num_features, boundary_letter):
    pad = np.zeros(shape=max_num_features)
    pads = [pad for _ in  np.arange(pad_size)]
    data = pads + data + pads
    flattened_context_windows = []
    for lower_bound in np.arange(len(data) - pad_size * 2):
        flattened_context_window = []
        context_window_size = pad_size * 2 + 1
        upper_bound = lower_bound + context_window_size
        context_window = data[lower_bound:upper_bound]
        for word in context_window:
            flattened_context_window.append([boundary_letter])
            flattened_context_window.append(word)
        flattened_context_window.append([boundary_letter])
        flattened_context_windows.append([int(word) for row in flattened_context_window for word in row])
    return flattened_context_windows


max_num_features = 10
space_letter = 0
space_padded_tokens = []
training_data = pd.read_csv("en_train.csv")
max_data_size = len(training_data)
print(training_data['class'].unique())
encoded_classes = pd.factorize(training_data['class'])
x_data = []
labels = encoded_classes[1]
y_data = encoded_classes[0]
gc.collect()
count = 0

for before_value in training_data['before'].values:
    row = np.ones(max_num_features, dtype=int) * space_letter
    for before_value_char, i in zip(list(str(before_value)), np.arange(max_num_features)):
        row[i] = ord(before_value_char)
    count+=1
    x_data.append(row)

x_data = x_data[:max_data_size]
y_data = y_data[:max_data_size]
x_data = np.array(make_flat_context_windows(x_data, pad_size = 1, max_num_features= max_num_features, boundary_letter=-1))
gc.collect()

x_train = np.array(x_data)
y_train = np.array(y_data)
gc.collect()

x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1)
num_classes = len(labels)

y_train = keras.utils.to_categorical(y_train, num_classes)
y_valid = keras.utils.to_categorical(y_valid, num_classes)
print(x_train.shape)

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_valid = np.reshape(x_valid, (x_valid.shape[0], x_valid.shape[1], 1))

model = Sequential()
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=((max_num_features * 3) + 4, 1)))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(labels), activation='softmax'))

model.summary()

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=256, epochs=50, verbose=1, validation_data=(x_valid, y_valid))

gc.collect()

score = model.evaluate(x_valid, y_valid, verbose=0)
print("Accuracy on validaton dataset")
print(score[1])

plt.plot(history.history['acc'])
plt.title("Accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.show()

plt.plot(history.history['loss'])
plt.title('Loss')
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.show()

predicted_valid = model.predict(x_valid)
predicted_valid = [labels[np.argmax(x)] for x in predicted_valid]
x_valid = [[chr(x) for x in y[2 + max_num_features: 2 + max_num_features * 2]] for y in x_valid]
x_valid = [''.join(x) for x in x_valid]
x_valid = [re.sub('a+$', '', x) for x in x_valid]
gc.collect()

df_predicted_valid = pd.DataFrame(columns=['data', 'predict'])
df_predicted_valid['data'] = x_valid
df_predicted_valid['predict'] = predicted_valid
df_predicted_valid.to_csv('validation_pred_lstm.csv')

df_predicted_valid.head()

test_dataset = pd.read_csv("en_test.csv")
test_data = test_dataset['before'].values

test_data = make_encoded_space_padded_tokens(data=test_data, max_num_features=max_num_features, space_char=0)
test_data = np.array(make_flat_context_windows(data=test_data, pad_size=1, max_num_features=max_num_features, boundary_letter=-1))

test_data = np.reshape(test_data, (test_data.shape[0], test_data.shape[1], 1))

predicted_data = model.predict(test_data)

predicted_data = [labels[np.argmax(char)] for char in predicted_data]
test_data = [[chr(wrd) for wrd in char[2 + max_num_features: 2 + max_num_features * 2]] for char in test_data]
test_data = [''.join(wrd) for wrd in test_data]
test_data = [re.sub('a+$', '', wrd) for wrd in test_data]

gc.collect()

df_predicted = pd.DataFrame(columns=['data', 'predict'])
df_predicted['data'] = test_data
df_predicted['predict'] = predicted_data
df_predicted.to_csv('pred_lstm.csv')

all_classes = set(training_data['class'].unique())
predicted_classes = set(df_predicted['predict'].unique())
missing_classes = all_classes - predicted_classes

print("Missing Classes:")
for cls in missing_classes: print(cls)



from pathlib import Path
from sklearn.model_selection import train_test_split
from pprint import pprint
import pandas as pd
import numpy as np
import xgboost as xgb
import gc
import sys

usage = 'XGBoost.py /path/to/input/file.csv /path/to/save/model/'

if len(sys.argv) < 3:
    print('not enough arguments\n')
    print(usage)
    sys.exit(1)
if len(sys.argv) > 3:
    print('too many arguments\n')
    print(usage)
    sys.exit(1)


def main(argv):
    input_data_path = Path(argv[1])
    model_output_data_path = Path(argv[2])
    training_data = pd.read_csv(input_data_path)

    max_num_features = 10

    space_padded_tokens = []
    #change classes to array of numeric encodings of classes
    encoded_classes = pd.factorize(training_data['class'])
    #get pandas index (basically a special array) of label names for the encoded classes
    labels = encoded_classes[1]
    #write indexed labels. This ordering will be needed to decode any predictions made by this model
    labels.to_frame(index=False).to_csv(path_or_buf=str(Path('label_index.csv')), header=False)
    gc.collect()

    training_data = make_encoded_space_padded_tokens(
        data=training_data['before'].values,
        max_num_features=10,
        space_char=0
    )

    training_data = np.array(make_flat_context_windows(
        data=training_data,
        pad_size=1,
        max_num_features=max_num_features,
        boundary_letter=-1))

    x_train = training_data
    y_train = encoded_classes[0]
    gc.collect()

    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=1045)
    gc.collect()
    num_class = len(labels)
    dtrain = xgb.DMatrix(x_train, label=y_train)
    dvalid = xgb.DMatrix(x_valid, label=y_valid)
    watchlist = [(dvalid, 'valid'), (dtrain, 'train')]

    xgbregressor_params = {
        'objective':'multi:softmax',
        'learning_rate':'0.3',
        'max_depth':10,
        'silent':False,
        'n_jobs':64, #num threads to use
        'num_class':num_class,
        'eval_metric':'merror'
    }
    model = xgb.train(params=xgbregressor_params,
                      dtrain=dtrain,
                      num_boost_round=50,
                      evals=watchlist,
                      early_stopping_rounds=20,
                      verbose_eval=1)
    gc.collect()

    encoded_predictions = model.predict(dvalid)
    predictions = list()
    observations = list()
    for encoded_prediction, encoded_observation in zip(encoded_predictions, y_valid):
        prediction = labels[int(encoded_prediction)]
        observation = labels[int(encoded_observation)]
        predictions.append(prediction)
        observations.append(observation)

    gc.collect()

    model.save_model(model_output_data_path.joinpath(Path('booster_model')).name)

def make_encoded_space_padded_tokens(data, max_num_features, space_char):
    space_padded_tokens = list()
    for before_value in data:
        #initialize array of space characters
        space_padded_token = np.ones(max_num_features, dtype=int) * space_char
        #split the token (word) into a list of characters (much like a string in C)
        before_value_c_str = list(str(before_value))
        for before_value_char, i in zip(before_value_c_str, range(max_num_features)):
            #get the unicode code point of given character
            space_padded_token[i] = ord(before_value_char)
        space_padded_tokens.append(space_padded_token)
    return space_padded_tokens

def make_flat_context_windows(data, pad_size, max_num_features, boundary_letter):
    #get array of zeros
    pad = np.zeros(shape=max_num_features)
    #create array of pad arrays
    pads = list()
    for temp in range(pad_size):
        pads.append(pad)
    #example of what we're doing:
    #0   123   0   01230
    #0 + 232 + 0 = 02320
    #0   321   0   03210
    data = pads + data + pads
    flattened_context_windows = []
    for lower_bound in range(len(data) - pad_size * 2):
        flattened_context_window = []
        #calc how many tokens (lines) to look at
        context_window_size = pad_size * 2 + 1
        upper_bound = lower_bound + context_window_size
        #get a window of the data (this is a frame that moves by 1 each iteration)
        context_window = data[lower_bound:upper_bound]
        for word in context_window:
            flattened_context_window.append(boundary_letter)
            flattened_context_window.extend(word)
        flattened_context_window.append(boundary_letter)
        #append window list to list of window lists
        flattened_context_windows.append(flattened_context_window)
    return flattened_context_windows


main(sys.argv)



from pathlib import Path
from pprint import pprint
import pandas as pd
import numpy as np
import xgboost as xgb
import sys

usage = 'applyBoosted.py /path/to/model/file /path/to/label_index/file.csv /path/to/input/file.csv /path/to/output/file.csv'

if len(sys.argv) < 5:
    print('not enough arguments\n')
    print(usage)
    sys.exit(1)
if len(sys.argv) > 5:
    print('too many arguments\n')
    print(usage)
    sys.exit(1)


def main(argv):
    model_filepath = Path(argv[1])
    label_encodings_filepath = Path(argv[2])
    input_data_path = Path(argv[3])
    output_data_path = Path(argv[4])

    max_num_features = 10

    model = xgb.Booster(model_file=str(model_filepath))

    xgb.plot_tree(model, fmap='', num_trees=0, rankdir='UT', ax=None)

    label_encodings = pd.read_csv(label_encodings_filepath, header=None, index_col=None)[1]
    print('label_encodings: ')
    pprint(label_encodings)

    input_data = pd.read_csv(input_data_path)
    test_data = input_data['before'].values
    test_data = make_encoded_space_padded_tokens(
        data=test_data,
        max_num_features=max_num_features,
        space_char=0
    )
    test_data = np.array(make_flat_context_windows(
        data=test_data,
        pad_size=1,
        max_num_features=max_num_features,
        boundary_letter=-1
    ))

    test_dm = xgb.DMatrix(data=test_data)
    encoded_predictions = model.predict(data=test_dm)

    labeled_data = decode_join(data=input_data, encoded_labels=encoded_predictions, label_encodings=label_encodings)
    print(labeled_data)
    labeled_data.to_csv(output_data_path)


def decode_join(data, encoded_labels, label_encodings):
    labels = list()
    for encoded_label in encoded_labels:
        label = label_encodings[int(encoded_label)]
        labels.append(label)
    labeled_data = data
    labeled_data['class'] = pd.Series(labels).values
    return labeled_data


def make_encoded_space_padded_tokens(data, max_num_features, space_char):
    space_padded_tokens = list()
    for before_value in data:
        #initialize array of space characters
        space_padded_token = np.ones(max_num_features, dtype=int) * space_char
        #split the token (word) into a list of characters (much like a string in C)
        before_value_c_str = list(str(before_value))
        for before_value_char, i in zip(before_value_c_str, range(max_num_features)):
            #get the unicode code point of given character
            space_padded_token[i] = ord(before_value_char)
        space_padded_tokens.append(space_padded_token)
    return space_padded_tokens


def make_flat_context_windows(data, pad_size, max_num_features, boundary_letter):
    #get array of zeros
    pad = np.zeros(shape=max_num_features)
    #create array of pad arrays
    pads = list()
    for temp in range(pad_size):
        pads.append(pad)
    #example of what we're doing:
    #0   123   0   01230
    #0 + 232 + 0 = 02320
    #0   321   0   03210
    data = pads + data + pads
    flattened_context_windows = []
    for lower_bound in range(len(data) - pad_size * 2):
        flattened_context_window = []
        #calc how many tokens (lines) to look at
        context_window_size = pad_size * 2 + 1
        upper_bound = lower_bound + context_window_size
        #get a window of the data (this is a frame that moves by 1 each iteration)
        context_window = data[lower_bound:upper_bound]
        for word in context_window:
            flattened_context_window.append(boundary_letter)
            flattened_context_window.extend(word)
        flattened_context_window.append(boundary_letter)
        #append window list to list of window lists
        flattened_context_windows.append(flattened_context_window)
    return flattened_context_windows


main(sys.argv)


import pandas as pd
import numpy
import matplotlib
import matplotlib.pyplot as plt

%matplotlib inline
df = pd.read_csv("input/en_train.csv")
print(df.shape)
df_classes = df.groupby('class').count()
df_class_list = list(set(c for c in df['class']))
print(df_class_list)
x =[val for val in  df_classes['before']]
print (x)
df_classes_counts = df['class'].value_counts()
df_classes_counts.plot(kind='bar', rot=75)
df1= df[df['class']!='PLAIN']
df1 = df1[df1['class']!='PUNCT']
df_classes_counts = df1['class'].value_counts()
df_classes_counts.plot(kind='bar', rot=75)
train_data = df
group_class = train_data.groupby('class', as_index=False).count()
change_data = train_data.loc[train_data["before"] != train_data["after"]]
group_class_change = change_data.groupby('class', as_index=False).count()

percent_change = dict.fromkeys(["class", "percentage"])
percent_change["class"] = []; percent_change["percentage"] = []

for i in group_class_change["class"]:
    val_1 = list(group_class_change.loc[group_class_change["class"] == i]["before"])[0]
    val_2 = list(group_class.loc[group_class["class"] == i]["before"])[0]
    percentage = 100 * val_1/val_2
    percent_change["class"].append(i)
    percent_change["percentage"].append(percentage)
percent_change["class"].append("PUNCT")
percent_change["percentage"].append(0.00000)

percent_change = pd.DataFrame(percent_change)
print(percent_change)

percent_change.plot(kind="bar", x="class", y="percentage")

import pandas as pd
import numpy as np
import random
from matplotlib import pyplot as plt
import plotly.plotly as py
%matplotlib inline

train_data = pd.read_csv('input/en_train.csv')
print(train_data['class'].unique())
class_value = train_data['class'].value_counts()
train_data['class'].value_counts().plot(kind  = "bar")
##Check Null values
print("Null values per column")
print(train_data.isnull().sum(axis = 0))
print("\nNon-null values per column")
print(train_data.count())


from pathlib import Path
from sklearn.model_selection import train_test_split
from pprint import pprint
import pandas as pd
import numpy as np
import sys
import gc
import keras
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import RMSprop

usage = 'NNWithClassAndContext.py /path/to/input/file.csv /path/to/save/model/'

if len(sys.argv) < 3:
    print('not enough arguments\n')
    print(usage)
    sys.exit(1)
if len(sys.argv) > 3:
    print('too many arguments\n')
    print(usage)
    sys.exit(1)


def main(argv):
    input_data_path = Path(argv[1])
    model_output_data_path = Path(argv[2])
    training_data = pd.read_csv(input_data_path)

    max_num_features = 10

    space_padded_tokens = []
    #change classes to array of numeric encodings of classes
    encoded_classes = pd.factorize(training_data['class'])
    #get pandas index (basically a special array) of label names for the encoded classes
    labels = encoded_classes[1]
    #write indexed labels. This ordering will be needed to decode any predictions made by this model
    labels.to_frame(index=False).to_csv(path_or_buf=str(Path('label_index.csv')), header=False)
    gc.collect()

    preprocessed_before_tokens = make_encoded_space_padded_tokens(
        data=training_data['before'].values,
        max_num_features=10,
        space_char=0
    )

    preprocessed_before_tokens = pd.DataFrame(make_flat_context_windows(
        data=preprocessed_before_tokens,
        pad_size=1,
        max_num_features=max_num_features,
        boundary_letter=-1))

    preprocessed_after_tokens = make_encoded_space_padded_tokens(
        data=training_data['after'].values,
        max_num_features=50,
        space_char=0
    )

    x_train = pd.DataFrame(keras.utils.to_categorical(encoded_classes[0], len(labels)), columns=labels)
    x_train = x_train.join(preprocessed_before_tokens, lsuffix='', rsuffix='_before')
#    x_train['sentence_id'] = training_data['sentence_id']
    y_train = pd.DataFrame(preprocessed_after_tokens)

    gc.collect()

    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=1045)
    gc.collect()

    x_train = x_train.as_matrix()
    x_valid = x_valid.as_matrix()
    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
    x_valid = np.reshape(x_valid, (x_valid.shape[0], x_valid.shape[1], 1))

    model = Sequential()
    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=((max_num_features * 3) + 4 + len(labels), 1)))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(y_train.shape[1], activation='softmax'))

    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(x_train, y_train, batch_size=256, epochs=50, verbose=1, validation_data=(x_valid, y_valid))

    score = model.evaluate(x_valid, y_valid, verbose=0)
    print("Accuracy on validaton dataset")
    print(score[1])


    #model.save_model(model_output_data_path.joinpath(Path('booster_model')).name)

def make_encoded_space_padded_tokens(data, max_num_features, space_char):
    space_padded_tokens = list()
    for before_value in data:
        #initialize array of space characters
        space_padded_token = np.ones(max_num_features, dtype=int) * space_char
        #split the token (word) into a list of characters (much like a string in C)
        before_value_c_str = list(str(before_value))
        for before_value_char, i in zip(before_value_c_str, range(max_num_features)):
            #get the unicode code point of given character
            space_padded_token[i] = ord(before_value_char)
        space_padded_tokens.append(space_padded_token)
    return space_padded_tokens

def make_flat_context_windows(data, pad_size, max_num_features, boundary_letter):
    #get array of zeros
    pad = np.zeros(shape=max_num_features)
    #create array of pad arrays
    pads = list()
    for temp in range(pad_size):
        pads.append(pad)
    #example of what we're doing:
    #0   123   0   01230
    #0 + 232 + 0 = 02320
    #0   321   0   03210
    data = pads + data + pads
    flattened_context_windows = []
    for lower_bound in range(len(data) - pad_size * 2):
        flattened_context_window = []
        #calc how many tokens (lines) to look at
        context_window_size = pad_size * 2 + 1
        upper_bound = lower_bound + context_window_size
        #get a window of the data (this is a frame that moves by 1 each iteration)
        context_window = data[lower_bound:upper_bound]
        for word in context_window:
            flattened_context_window.append(boundary_letter)
            flattened_context_window.extend(word)
        flattened_context_window.append(boundary_letter)
        #append window list to list of window lists
        flattened_context_windows.append(flattened_context_window)
    return flattened_context_windows





main(sys.argv)




# coding: utf-8

# In[2]:


from sklearn.model_selection import train_test_split
from pprint import pprint
import pandas as pd
import numpy as np
import sys
import os
import gc
import re
import keras
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import RMSprop
import warnings
from sklearn.neural_network import MLPClassifier
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
warnings.filterwarnings("ignore")

training_data = pd.read_csv("en_train.csv").sample(n=100000,random_state=40)
batch_size = 128
epochs = 5
max_num_features = 10
space_letter = 0
space_padded_tokens = []
max_data_size = len(training_data)

encoded_classes = pd.factorize(training_data['class'])
# print_data = 100000
x_data = []
labels = encoded_classes[1]
y_data = encoded_classes[0]
gc.collect()
count = 0

for before_value in training_data['before'].values:
    row = np.ones(max_num_features, dtype=int) * space_letter
    for before_value_char, i in zip(list(str(before_value)), np.arange(max_num_features)):
        row[i] = ord(before_value_char)
    count+=1
    x_data.append(row)
    
def make_flat_context_windows(data, pad_size, max_num_features, boundary_letter):
    pad = np.zeros(shape=max_num_features)
    #create array of pad arrays
    pads = [pad for _ in  np.arange(pad_size)]
    data = pads + data + pads
    flattened_context_windows = []
    for lower_bound in np.arange(len(data) - pad_size * 2):
        flattened_context_window = []
        
        context_window_size = pad_size * 2 + 1
        upper_bound = lower_bound + context_window_size
        context_window = data[lower_bound:upper_bound]
        
        for word in context_window:
            flattened_context_window.append([boundary_letter])
            flattened_context_window.append(word)
        flattened_context_window.append([boundary_letter])
        
        flattened_context_windows.append([int(word) for row in flattened_context_window for word in row])
    return flattened_context_windows


x_data = x_data[:max_data_size]
y_data = y_data[:max_data_size]
x_data = np.array(make_flat_context_windows(x_data, pad_size = 1, max_num_features= max_num_features, boundary_letter=-1))
gc.collect()

x_train = np.array(x_data)
y_train = np.array(y_data)
gc.collect()

#adding before and after columns to the data so that it can be used later
b = np.zeros((x_train.shape[0], x_train.shape[1]+1), dtype='O')
b[:,:-1] = x_train
b[:,-1] = np.array(training_data['before'].tolist())

c = np.zeros((x_train.shape[0], 2), dtype='O')
c[:,0] = np.array(y_train.tolist())
c[:,1] = np.array(training_data['after'].tolist())

x_train, x_valid, y_train, y_valid = train_test_split(b, c, test_size=0.2, random_state=2017)

num_classes = len(labels)
y_train1 = keras.utils.to_categorical(y_train[:,:-1], num_classes)
y_valid1 = keras.utils.to_categorical(y_valid[:,:-1], num_classes)
x_train1 = np.reshape(x_train[:,:-1], (x_train[:,:-1].shape[0], x_train[:,:-1].shape[1], 1))
x_valid1 = np.reshape(x_valid[:,:-1], (x_valid[:,:-1].shape[0], x_valid[:,:-1].shape[1], 1))

model = Sequential()
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=((max_num_features * 3) + 4, 1)))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(labels), activation='softmax'))

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train1, y_train1,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(x_valid1, y_valid1))


# In[3]:


#Storing all the attributes in one dataframe
a=model.predict(x_train1)
trainDatax=pd.DataFrame(x_train[:,:-1])
trainDatax['predict_class']=pd.DataFrame(np.argmax(a, axis=1).tolist())
trainDatax['before']=pd.DataFrame(x_train[:,-1])

trainDatay=pd.DataFrame(y_train)
trainDatay.columns = ['class','after']

#Change is 0 if before=column else change is 1
trainDatay['change'] = 1
trainDatay['before'] = trainDatax['before']
trainDatay.loc[trainDatay.before == trainDatay.after, 'change'] = 0
trainDatax['predict_class'] = trainDatay['class']

#Splitting data into its respective classes
x_train2 = [0 for x in range(len(labels))]
y_train2 = [0 for x in range(len(labels))]
model1 = [0 for x in range(len(labels))]
model2 = [0 for x in range(len(labels))]
for x in range(len(labels)):
    x_train2[x] = trainDatax[trainDatay['class']==x]
    y_train2[x] = trainDatay[trainDatay['class']==x]
    
    if x_train2[x].shape[0]==0:
        continue
    
    #Input Columns that are needed for predicting change
    list1 = []
    for y in range(x_train2[x].shape[1]-2):
        list1.append(y)
    list1.append('predict_class')
    
    #Training for predicting if change is needed
    model1[x] = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10,10), activation='tanh',random_state=60)
    model1[x].fit(x_train2[x][list1],y_train2[x][['change']])
    
    y_d = []
    max_num_features=30
    for after_value in y_train2[x]['after'].values:
        row = np.ones(max_num_features, dtype=int) * space_letter
        for after_value_char, i in zip(list(str(after_value)), np.arange(max_num_features)):
            row[i] = ord(after_value_char)
        y_d.append(row)
    
    #if predicted change is 1,then predict the after column
    model2[x] = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(100,100,100,100,100), activation='tanh',random_state=120)
    model2[x].fit(x_train2[x][list1],y_d)


# In[15]:


def testing(X_test,y_test,labels):
    #Predicting class
    x_valid2 = np.reshape(X_test[:,:-1], (X_test[:,:-1].shape[0], X_test[:,:-1].shape[1], 1))
    a=model.predict(x_valid2)
    testDatax=pd.DataFrame(X_test[:,:-1])
    testDatax['predict_class']=pd.DataFrame(np.argmax(a, axis=1).tolist())
    testDatax['before']=pd.DataFrame(X_test[:,-1])
    
    testDatay=pd.DataFrame(y_test)
    testDatay.columns = ['class','after']
    
    #Splitting Data
    x_test1 = [0 for x in range(len(labels))]
    y_test1 = [0 for x in range(len(labels))]
    for x in range(len(labels)):
        print(x)
        x_test1[x] = testDatax[testDatax['predict_class']==x]
        y_test1[x] = testDatay[testDatax['predict_class']==x]

        if x_test1[x].shape[0]==0:
            continue
        
        list1 = []
        for y in range(x_test1[x].shape[1]-2):
            list1.append(y)
        list1.append('predict_class')
        
        #Predicting change
        x_test1[x]['change_predict'] = model1[x].predict(x_test1[x][list1]).astype(int)
        x_test1[x]['predict'] = x_test1[x]['change_predict']
        x_test1[x].loc[x_test1[x].change_predict == 0, 'predict'] = x_test1[x].loc[x_test1[x].change_predict == 0, 'before']
        if x_test1[x][x_test1[x]['change_predict']==1].shape[0]==0:
            continue
        
        #Predicting Output if predicted change is 1
        p = model2[x].predict(x_test1[x][x_test1[x]['change_predict']==1][list1]).astype(int)
        p = np.array(p)
        
        row=0
        for y in x_test1[x][x_test1[x]['change_predict']==1].index.tolist():
            val = ""
            for col in range(p.shape[1]):
                try:
                    if p[row][col]>31 and p[row][col]<58:
                        val = val + chr(p[row][col])
                    elif p[row][col]>64 and p[row][col]<91:
                        val = val + chr(p[row][col])
                    elif p[row][col]>96 and p[row][col]<123:
                        val = val + chr(p[row][col])
                except:
                    continue
            
            x_test1[x].loc[y,'predict'] = val
            row=row+1
        
        
    input1=x_test1[0]
    output1=y_test1[0]
    for x in range(1,len(labels)):
        input1 =pd.concat([input1,x_test1[x]])
        output1 =pd.concat([output1,y_test1[x]])
    
    input1 = input1[['before','predict','predict_class']]
    output1 = output1['after']
    
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
    print("Accuracy : "+str(input1[input1['predict']==output1].shape[0]*100/input1.shape[0]))
    print("precision_score : "+str(precision_score(output1, input1['predict'], average="macro")))
    print("recall_score : "+str(recall_score(output1, input1['predict'], average="macro")))
    print("f1_score : "+str(f1_score(output1, input1['predict'], average="macro")))
    return input1,output1


# In[16]:


print("Testing : ")
print("Number of instances : "+str(x_valid.shape[0]))
input2,output2=testing(x_valid,y_valid,labels)


# In[17]:


print("Training : ")
print("Number of instances : "+str(x_train.shape[0]))
input1,output1=testing(x_train,y_train,labels)


# In[26]:


input2['after']=output2
input1['after']=output1

class_transform={}
for x in range(len(labels)):
    class_transform[x]=labels[x]

input1['predict_class']=input1['predict_class'].apply(class_transform.get).astype(str)
input2['predict_class']=input2['predict_class'].apply(class_transform.get).astype(str)

input1.to_csv('pred_train.csv')
input2.to_csv('pred_test.csv')
