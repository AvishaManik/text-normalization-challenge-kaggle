{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/5\n",
      "80000/80000 [==============================] - 58s 729us/step - loss: 0.4862 - acc: 0.8810 - val_loss: 0.5810 - val_acc: 0.8029\n",
      "Epoch 2/5\n",
      "80000/80000 [==============================] - 51s 640us/step - loss: 0.2497 - acc: 0.9397 - val_loss: 0.5401 - val_acc: 0.8119\n",
      "Epoch 3/5\n",
      "80000/80000 [==============================] - 54s 681us/step - loss: 0.2107 - acc: 0.9492 - val_loss: 0.5047 - val_acc: 0.8089\n",
      "Epoch 4/5\n",
      "80000/80000 [==============================] - 49s 613us/step - loss: 0.1899 - acc: 0.9545 - val_loss: 0.2527 - val_acc: 0.9116\n",
      "Epoch 5/5\n",
      "80000/80000 [==============================] - 48s 604us/step - loss: 0.1783 - acc: 0.9581 - val_loss: 0.1820 - val_acc: 0.9490\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import warnings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "training_data = pd.read_csv(\"en_train.csv\").sample(n=100000,random_state=40)\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "max_num_features = 10\n",
    "space_letter = 0\n",
    "space_padded_tokens = []\n",
    "max_data_size = len(training_data)\n",
    "\n",
    "encoded_classes = pd.factorize(training_data['class'])\n",
    "# print_data = 100000\n",
    "x_data = []\n",
    "labels = encoded_classes[1]\n",
    "y_data = encoded_classes[0]\n",
    "gc.collect()\n",
    "count = 0\n",
    "\n",
    "for before_value in training_data['before'].values:\n",
    "    row = np.ones(max_num_features, dtype=int) * space_letter\n",
    "    for before_value_char, i in zip(list(str(before_value)), np.arange(max_num_features)):\n",
    "        row[i] = ord(before_value_char)\n",
    "    count+=1\n",
    "    x_data.append(row)\n",
    "    \n",
    "def make_flat_context_windows(data, pad_size, max_num_features, boundary_letter):\n",
    "    pad = np.zeros(shape=max_num_features)\n",
    "    #create array of pad arrays\n",
    "    pads = [pad for _ in  np.arange(pad_size)]\n",
    "    data = pads + data + pads\n",
    "    flattened_context_windows = []\n",
    "    for lower_bound in np.arange(len(data) - pad_size * 2):\n",
    "        flattened_context_window = []\n",
    "        \n",
    "        context_window_size = pad_size * 2 + 1\n",
    "        upper_bound = lower_bound + context_window_size\n",
    "        context_window = data[lower_bound:upper_bound]\n",
    "        \n",
    "        for word in context_window:\n",
    "            flattened_context_window.append([boundary_letter])\n",
    "            flattened_context_window.append(word)\n",
    "        flattened_context_window.append([boundary_letter])\n",
    "        \n",
    "        flattened_context_windows.append([int(word) for row in flattened_context_window for word in row])\n",
    "    return flattened_context_windows\n",
    "\n",
    "\n",
    "x_data = x_data[:max_data_size]\n",
    "y_data = y_data[:max_data_size]\n",
    "x_data = np.array(make_flat_context_windows(x_data, pad_size = 1, max_num_features= max_num_features, boundary_letter=-1))\n",
    "gc.collect()\n",
    "\n",
    "x_train = np.array(x_data)\n",
    "y_train = np.array(y_data)\n",
    "gc.collect()\n",
    "\n",
    "#adding before and after columns to the data so that it can be used later\n",
    "b = np.zeros((x_train.shape[0], x_train.shape[1]+1), dtype='O')\n",
    "b[:,:-1] = x_train\n",
    "b[:,-1] = np.array(training_data['before'].tolist())\n",
    "\n",
    "c = np.zeros((x_train.shape[0], 2), dtype='O')\n",
    "c[:,0] = np.array(y_train.tolist())\n",
    "c[:,1] = np.array(training_data['after'].tolist())\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(b, c, test_size=0.2, random_state=2017)\n",
    "\n",
    "num_classes = len(labels)\n",
    "y_train1 = keras.utils.to_categorical(y_train[:,:-1], num_classes)\n",
    "y_valid1 = keras.utils.to_categorical(y_valid[:,:-1], num_classes)\n",
    "x_train1 = np.reshape(x_train[:,:-1], (x_train[:,:-1].shape[0], x_train[:,:-1].shape[1], 1))\n",
    "x_valid1 = np.reshape(x_valid[:,:-1], (x_valid[:,:-1].shape[0], x_valid[:,:-1].shape[1], 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=((max_num_features * 3) + 4, 1)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train1, y_train1,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_valid1, y_valid1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Storing all the attributes in one dataframe\n",
    "a=model.predict(x_train1)\n",
    "trainDatax=pd.DataFrame(x_train[:,:-1])\n",
    "trainDatax['predict_class']=pd.DataFrame(np.argmax(a, axis=1).tolist())\n",
    "trainDatax['before']=pd.DataFrame(x_train[:,-1])\n",
    "\n",
    "trainDatay=pd.DataFrame(y_train)\n",
    "trainDatay.columns = ['class','after']\n",
    "\n",
    "#Change is 0 if before=column else change is 1\n",
    "trainDatay['change'] = 1\n",
    "trainDatay['before'] = trainDatax['before']\n",
    "trainDatay.loc[trainDatay.before == trainDatay.after, 'change'] = 0\n",
    "trainDatax['predict_class'] = trainDatay['class']\n",
    "\n",
    "#Splitting data into its respective classes\n",
    "x_train2 = [0 for x in range(len(labels))]\n",
    "y_train2 = [0 for x in range(len(labels))]\n",
    "model1 = [0 for x in range(len(labels))]\n",
    "model2 = [0 for x in range(len(labels))]\n",
    "for x in range(len(labels)):\n",
    "    x_train2[x] = trainDatax[trainDatay['class']==x]\n",
    "    y_train2[x] = trainDatay[trainDatay['class']==x]\n",
    "    \n",
    "    if x_train2[x].shape[0]==0:\n",
    "        continue\n",
    "    \n",
    "    #Input Columns that are needed for predicting change\n",
    "    list1 = []\n",
    "    for y in range(x_train2[x].shape[1]-2):\n",
    "        list1.append(y)\n",
    "    list1.append('predict_class')\n",
    "    \n",
    "    #Training for predicting if change is needed\n",
    "    model1[x] = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10,10), activation='tanh',random_state=60)\n",
    "    model1[x].fit(x_train2[x][list1],y_train2[x][['change']])\n",
    "    \n",
    "    y_d = []\n",
    "    max_num_features=30\n",
    "    for after_value in y_train2[x]['after'].values:\n",
    "        row = np.ones(max_num_features, dtype=int) * space_letter\n",
    "        for after_value_char, i in zip(list(str(after_value)), np.arange(max_num_features)):\n",
    "            row[i] = ord(after_value_char)\n",
    "        y_d.append(row)\n",
    "    \n",
    "    #if predicted change is 1,then predict the after column\n",
    "    model2[x] = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(100,100,100,100,100), activation='tanh',random_state=120)\n",
    "    model2[x].fit(x_train2[x][list1],y_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(X_test,y_test,labels):\n",
    "    #Predicting class\n",
    "    x_valid2 = np.reshape(X_test[:,:-1], (X_test[:,:-1].shape[0], X_test[:,:-1].shape[1], 1))\n",
    "    a=model.predict(x_valid2)\n",
    "    testDatax=pd.DataFrame(X_test[:,:-1])\n",
    "    testDatax['predict_class']=pd.DataFrame(np.argmax(a, axis=1).tolist())\n",
    "    testDatax['before']=pd.DataFrame(X_test[:,-1])\n",
    "    \n",
    "    testDatay=pd.DataFrame(y_test)\n",
    "    testDatay.columns = ['class','after']\n",
    "    \n",
    "    #Splitting Data\n",
    "    x_test1 = [0 for x in range(len(labels))]\n",
    "    y_test1 = [0 for x in range(len(labels))]\n",
    "    for x in range(len(labels)):\n",
    "        print(x)\n",
    "        x_test1[x] = testDatax[testDatax['predict_class']==x]\n",
    "        y_test1[x] = testDatay[testDatax['predict_class']==x]\n",
    "\n",
    "        if x_test1[x].shape[0]==0:\n",
    "            continue\n",
    "        \n",
    "        list1 = []\n",
    "        for y in range(x_test1[x].shape[1]-2):\n",
    "            list1.append(y)\n",
    "        list1.append('predict_class')\n",
    "        \n",
    "        #Predicting change\n",
    "        x_test1[x]['change_predict'] = model1[x].predict(x_test1[x][list1]).astype(int)\n",
    "        x_test1[x]['predict'] = x_test1[x]['change_predict']\n",
    "        x_test1[x].loc[x_test1[x].change_predict == 0, 'predict'] = x_test1[x].loc[x_test1[x].change_predict == 0, 'before']\n",
    "        if x_test1[x][x_test1[x]['change_predict']==1].shape[0]==0:\n",
    "            continue\n",
    "        \n",
    "        #Predicting Output if predicted change is 1\n",
    "        p = model2[x].predict(x_test1[x][x_test1[x]['change_predict']==1][list1]).astype(int)\n",
    "        p = np.array(p)\n",
    "        \n",
    "        row=0\n",
    "        for y in x_test1[x][x_test1[x]['change_predict']==1].index.tolist():\n",
    "            val = \"\"\n",
    "            for col in range(p.shape[1]):\n",
    "                try:\n",
    "                    if p[row][col]>31 and p[row][col]<58:\n",
    "                        val = val + chr(p[row][col])\n",
    "                    elif p[row][col]>64 and p[row][col]<91:\n",
    "                        val = val + chr(p[row][col])\n",
    "                    elif p[row][col]>96 and p[row][col]<123:\n",
    "                        val = val + chr(p[row][col])\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            x_test1[x].loc[y,'predict'] = val\n",
    "            row=row+1\n",
    "        \n",
    "        \n",
    "    input1=x_test1[0]\n",
    "    output1=y_test1[0]\n",
    "    for x in range(1,len(labels)):\n",
    "        input1 =pd.concat([input1,x_test1[x]])\n",
    "        output1 =pd.concat([output1,y_test1[x]])\n",
    "    \n",
    "    input1 = input1[['before','predict','predict_class']]\n",
    "    output1 = output1['after']\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    print(\"Accuracy : \"+str(input1[input1['predict']==output1].shape[0]*100/input1.shape[0]))\n",
    "    print(\"precision_score : \"+str(precision_score(output1, input1['predict'], average=\"macro\")))\n",
    "    print(\"recall_score : \"+str(recall_score(output1, input1['predict'], average=\"macro\")))\n",
    "    print(\"f1_score : \"+str(f1_score(output1, input1['predict'], average=\"macro\")))\n",
    "    return input1,output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing : \n",
      "Number of instances : 20000\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Accuracy : 91.425\n",
      "precision_score : 0.847872700021\n",
      "recall_score : 0.842445306067\n",
      "f1_score : 0.843961819494\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing : \")\n",
    "print(\"Number of instances : \"+str(x_valid.shape[0]))\n",
    "input2,output2=testing(x_valid,y_valid,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : \n",
      "Number of instances : 80000\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Accuracy : 91.3375\n",
      "precision_score : 0.835756464047\n",
      "recall_score : 0.830320787696\n",
      "f1_score : 0.831680517723\n"
     ]
    }
   ],
   "source": [
    "print(\"Training : \")\n",
    "print(\"Number of instances : \"+str(x_train.shape[0]))\n",
    "input1,output1=testing(x_train,y_train,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
